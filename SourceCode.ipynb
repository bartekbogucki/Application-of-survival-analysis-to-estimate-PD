{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lifelines\n",
    "from lifelines import CoxPHFitter, WeibullAFTFitter, LogNormalAFTFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "#pycox\n",
    "from pycox.models import DeepHitSingle\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "#others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from category_encoders import CatBoostEncoder\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_zero_values_table(df):\n",
    "    # Calculate 0 and missing values for every variable of input pandas df\n",
    "    zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n",
    "    mz_table = mz_table.rename(\n",
    "    columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n",
    "    mz_table['Data Type'] = df.dtypes\n",
    "    mz_table = mz_table[\n",
    "        mz_table.iloc[:,1] != 0].sort_values(\n",
    "    '% of Total Values', ascending=False).round(1)\n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "        \"There are \" + str(mz_table.shape[0]) +\n",
    "        \" columns that have missing values.\")\n",
    "    return mz_table\n",
    "    \n",
    "def preprocessing(sample):\n",
    "    # Select catrgorical variables\n",
    "    cats = [c for c in sample.columns if sample[c].dtypes == 'object']\n",
    "    # Catboost encoder for categorical variables\n",
    "    CBE_encoder = CatBoostEncoder()\n",
    "    sample[cats] = CBE_encoder.fit_transform(sample[cats], sample['default_adj'])\n",
    "    # Linear interpolation for missing values\n",
    "    sample = sample.interpolate(method='linear', limit_direction='both')\n",
    "    # Min-max scaling of variables\n",
    "    scaler = MinMaxScaler()\n",
    "    sample[list(set(sample.columns).difference(['intodefault_12', 'default_adj', 'duration', 'cid']))] = scaler.fit_transform(\n",
    "        sample[list(set(sample.columns).difference(['intodefault_12', 'default_adj', 'duration', 'cid']))]\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "def format_quarterly_date(date):\n",
    "    # Create quarter format of date for visualisation purposes\n",
    "    return f\"{date.year}-Q{int((date.month-1) / 3) + 1}\"\n",
    "\n",
    "def evaluate_model_performance(sample, model, reporting=0):\n",
    "    # Evaluation of model discriminatory power by calculating AUC values\n",
    "    sample=sample.reset_index(drop=True)\n",
    "    if reporting==1:\n",
    "        sample=sample.drop(['reporting_date'], axis=1)\n",
    "    if model==logreg:\n",
    "        y_pred_proba = model.predict_proba(sample.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1))[:, 1]\n",
    "    else:\n",
    "        test_survival_probabilities = model.predict_surv_df(sample.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1).values.astype('float32')).reset_index(drop=True)\n",
    "        duration_column = sample['duration']+12\n",
    "        survival = pd.Series(test_survival_probabilities.T.lookup(sample.index, duration_column), index=sample.index)\n",
    "        y_pred_proba=1-survival\n",
    "    auc = roc_auc_score(sample['intodefault_12'], y_pred_proba)\n",
    "    return auc, c_index\n",
    "\n",
    "def integrated_calibration_index(y_true, y_prob, n_bins=10):\n",
    "    # Sort the true labels and predicted probabilities based on the ascending order of probabilities\n",
    "    sorted_indices = np.argsort(y_prob)\n",
    "    y_true_sorted = y_true[sorted_indices]\n",
    "    y_prob_sorted = y_prob[sorted_indices]\n",
    "\n",
    "    # Calculate the bin size based on the length of y_true and the specified number of bins\n",
    "    bin_size = len(y_true_sorted) // n_bins\n",
    "    \n",
    "    # Calculate the indices for dividing the sorted predictions into equal-sized bins\n",
    "    bin_indices = np.arange(1, n_bins + 1) * bin_size\n",
    "\n",
    "    # Calculate the proportion of positive labels in each bin\n",
    "    bin_true_prob = np.array([np.mean(y_true_sorted[i-bin_size:i]) for i in bin_indices])\n",
    "\n",
    "    # Calculate the mean predicted probability in each bin\n",
    "    bin_pred_prob = np.array([np.mean(y_prob_sorted[i-bin_size:i]) for i in bin_indices])\n",
    "\n",
    "    # Calculate the integrated calibration index\n",
    "    ici = np.sum(np.abs(bin_true_prob - bin_pred_prob)) / n_bins\n",
    "\n",
    "    return ici\n",
    "\n",
    "def plot_calibration_curve(y, probs, title):\n",
    "    # Brier Score\n",
    "    brier_score = brier_score_loss(y, probs)\n",
    "    # Integrated calibration index\n",
    "    ici=integrated_calibration_index(y, probs)\n",
    "    # Calculate proportion of positive labels and predicted probability in each of 10 bin (n_bins=10)\n",
    "    prob_true, prob_pred = calibration_curve(y, probs, n_bins=10)\n",
    "    # Plot calibration curve\n",
    "    plt.figure(figsize=(6, 6)) \n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.plot(\n",
    "        prob_pred,\n",
    "        prob_true,\n",
    "        marker=\".\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    plt.title(f\"{title}\\nBrier score: {round(brier_score, 4)}\\nICI: {round(ici, 4)}\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.xlabel(\"Mean predicted value\")\n",
    "    return prob_true, prob_pred\n",
    "\n",
    "def jeffreys_test(D, N, PD, confidence_level=0.95):\n",
    "    # Calculate shape parameters for the beta distribution\n",
    "    a = D + 0.5\n",
    "    b = N - D + 0.5\n",
    "    \n",
    "    # Calculate the p-value using the cumulative distribution function (CDF)\n",
    "    p_value = 1 - stats.beta.cdf(PD, a, b)\n",
    "    \n",
    "    # Calculate the Highest Density Interval (HDI) with the given confidence level\n",
    "    beta_distribution = stats.beta(a, b)\n",
    "    hdi = beta_distribution.interval(confidence_level)\n",
    "    \n",
    "    return p_value, hdi\n",
    "\n",
    "def calculate_observed_loss(sample_table, reporting_date, time_horizon):\n",
    "    # Calculation of observed loss for specified date and time horizon\n",
    "    horizon_date = reporting_date + relativedelta(years=time_horizon)\n",
    "    observed_loss = sample_table.loc[(sample_table['reporting_date'] == horizon_date) & (sample_table['default_adj'] == 1), 'Outstanding'].sum()\n",
    "    return observed_loss\n",
    "\n",
    "def calculate_corr(sample, sample_name, duration_ind=0):\n",
    "    sample=sample.reset_index(drop=True)\n",
    "    duration_column = sample['duration']+12\n",
    "    \n",
    "    # Logistic regression PD\n",
    "    LR_PD = pd.Series(logreg.predict_proba(sample.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1))[:, 1])\n",
    "    \n",
    "    # Weibull AFT\n",
    "    test_survival_probabilities = waft.predict_survival_function(sample, times=np.arange(1, 361))\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(sample.index, duration_column), index=sample.index)\n",
    "    waft_PD=1-survival\n",
    "    \n",
    "    # Log-Normal AFT\n",
    "    test_survival_probabilities = lognormaft.predict_survival_function(sample, times=np.arange(1, 361))\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(sample.index, duration_column), index=sample.index)\n",
    "    lognormaft_PD=1-survival\n",
    "\n",
    "    # Cox PH\n",
    "    test_survival_probabilities = cph.predict_survival_function(sample, times=np.arange(1, 361))\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(sample.index, duration_column), index=sample.index)\n",
    "    cph_PD=1-survival\n",
    "    \n",
    "    # DeepHit PD\n",
    "    test_survival_probabilities = model.predict_surv_df(sample.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1).values.astype('float32'))\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(sample.index, duration_column), index=sample.index)\n",
    "    DeepHit_PD=1-survival\n",
    "\n",
    "    # Correlation matrix between PD results obtained from all models\n",
    "    df_corr=pd.DataFrame({'Logistic regression': LR_PD, 'Weibull AFT':waft_PD, 'Log-Normal AFT': lognormaft_PD, 'Cox PH': cph_PD, 'DeepHit': DeepHit_PD})\n",
    "    corr_matrix, p_value = stats.spearmanr(df_corr, axis=0)\n",
    "    \n",
    "    # Correlation plot between PD results obtained from all models\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', xticklabels=df_corr.columns, yticklabels=df_corr.columns, fmt=\".4f\", cbar_kws={'label': 'Spearman Correlation Coefficient'})\n",
    "    plt.title('Correlation matrix between PD predictions')\n",
    "    plt.show()\n",
    "\n",
    "    # p-values\n",
    "    significant_mask=np.where(p_value<0.05,1,0)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    custom_pallette=[\"lightgreen\",\"lightcoral\"]\n",
    "    heatmap=sns.heatmap(significant_mask, annot=p_value, cmap=custom_pallette, xticklabels=df_corr.columns, yticklabels=df_corr.columns, fmt=\".2f\", cbar=False)\n",
    "    plt.title('P-values matrix between PD predictions')\n",
    "    plt.show()\n",
    "    \n",
    "    # Define the pairs of x and y values and their corresponding titles\n",
    "    pairs = [\n",
    "        (LR_PD, DeepHit_PD, 'Logistic regression PD', 'DeepHit PD'),\n",
    "        (lognormaft_PD, DeepHit_PD, 'Log-Normal AFT PD', 'DeepHit PD')\n",
    "    ]\n",
    "\n",
    "    # Iterate through the pairs and create scatterplots\n",
    "    for x, y, x_label, y_label in pairs:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(x, y)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        if duration_ind == 0:\n",
    "            plt.title(f'Scatterplot between {y_label} and {x_label} predictions for {sample_name} sample')\n",
    "        else:\n",
    "            plt.title(f'Scatterplot between {y_label} and {x_label} predictions for {sample_name} sample for duration {duration}')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return corr_matrix, p_value, DeepHit_PD, LR_PD, waft_PD, lognormaft_PD, cph_PD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data upload and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_final_final.csv\", delimiter=\",\")\n",
    "df.drop('default', axis=1, inplace=True)\n",
    "df.loc[(df['default_adj'] == 1) & (df['intodefault_12'] == 0), 'intodefault_12'] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating missing values\n",
    "missing_zero_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing value with 0 for the given variables\n",
    "df['rel_breach_pst_due_amt_o'] = df['rel_breach_pst_due_amt_o'].fillna(0)\n",
    "df['absolute_breach_past_due_amnt_o'] = df['absolute_breach_past_due_amnt_o'].fillna(0)\n",
    "df['diff_rel_breach_end_rep_date'] = df['diff_rel_breach_end_rep_date'].fillna(0)\n",
    "df['diff_abs_breach_end_rep_date'] = df['diff_abs_breach_end_rep_date'].fillna(0)\n",
    "df['diff_abs_breach_str_rep_date'] = df['diff_abs_breach_str_rep_date'].fillna(0)\n",
    "\n",
    "df=df.rename(columns={\"Average duration of existing installment credit lines. Calculated starting from the original durations of the individual credit lines.\": \"avg_duration_installment\"})\n",
    "\n",
    "# Datetime format for reporting_date\n",
    "df['reporting_date'] = pd.to_datetime(df['reporting_date'])\n",
    "df['min_rep_date'] = pd.to_datetime(df['min_rep_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessery variables for modelling purposes from df\n",
    "data_dev = df.drop(['Arrear_amount_adj','first_def_date','mx_rep_Date','default_start_date','default_exit_date', 'Outstanding'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split cid (facility id) into two groups\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 11)\n",
    "split = splitter.split(data_dev, groups=data_dev['cid'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "sample_1 = data_dev.iloc[train_inds].reset_index(drop=True)\n",
    "sample_2 = data_dev.iloc[test_inds].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, OOS, and OOT samples\n",
    "train = sample_1[sample_1['reporting_date'] < datetime(2019,7,1)].drop(['reporting_date', 'min_rep_date'], axis=1).reset_index(drop=True)\n",
    "test = sample_2[sample_2['reporting_date'] < datetime(2019,7,1)].drop(['reporting_date', 'min_rep_date'], axis=1).reset_index(drop=True)\n",
    "OOT_full = pd.concat([sample_2[sample_2['reporting_date'] >= datetime(2019, 7, 1)].drop(['reporting_date', 'min_rep_date'], axis=1),sample_1[sample_1['min_rep_date'] >= datetime(2019,7,1)].drop(['reporting_date', 'min_rep_date'], axis=1)],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of samples\n",
    "train=preprocessing(train)\n",
    "test=preprocessing(test)\n",
    "OOT_full=preprocessing(OOT_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OOT sample for comparing with logistic regression up to Dec 2020\n",
    "merged_OOT = pd.merge(OOT_full, df[['cid', 'duration', 'reporting_date']], on=['cid', 'duration'], how='left')\n",
    "OOT=merged_OOT[merged_OOT['reporting_date']<datetime(2021, 1, 1)].drop(['reporting_date'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X, y, t, id\n",
    "X_train_on, Y_train_on, T_train_on, ID_train_on = train.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1), train['default_adj'], train['duration'], train['cid']\n",
    "X_test_on, Y_test_on, T_test_on, ID_test_on = test.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1), test['default_adj'], test['duration'], test['cid']\n",
    "X_test_OOT, Y_test_OOT, T_test_OOT, ID_test_OOT = OOT.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1), OOT['default_adj'], OOT['duration'], OOT['cid']\n",
    "X_test_OOT_full, Y_test_OOT_full, T_test_OOT_full, ID_test_OOT_full = OOT_full.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1), OOT_full['default_adj'], OOT_full['duration'], OOT_full['cid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_on.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Modelling & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train_on, train['intodefault_12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_name, sample in [('train', train), ('test', test), ('OOT', OOT)]:\n",
    "    y_pred_proba = logreg.predict_proba(sample.reset_index(drop=True).drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1))[:, 1]\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    auc = roc_auc_score(sample['intodefault_12'], y_pred_proba)\n",
    "    print(f\"AUC  for {sample_name} sample: {auc :.4f}\")\n",
    "    c_index = concordance_index(sample['duration'], -y_pred_proba, sample['intodefault_12'])\n",
    "    print(f\"C-index for {sample_name} sample: {c_index :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sample_name, sample in [('train', train), ('test', test), ('OOT', OOT)]:\n",
    "    df_sample = sample.copy()\n",
    "    # Perform left join\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'Arrear_amount_adj']], on=['cid', 'duration'], how='left')\n",
    "    \n",
    "    sub_segment_0 = merged_table[merged_table['Arrear_amount_adj'] <= 0]\n",
    "    sub_segment_1 = merged_table[merged_table['Arrear_amount_adj'] > 0]\n",
    "    \n",
    "    y_pred_proba_0 = logreg.predict_proba(sub_segment_0.drop(['Arrear_amount_adj', 'intodefault_12', 'default_adj', 'duration', 'cid'], axis=1))[:, 1]\n",
    "    y_pred_proba_1 = logreg.predict_proba(sub_segment_1.drop(['Arrear_amount_adj', 'intodefault_12', 'default_adj', 'duration', 'cid'], axis=1))[:, 1]\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    auc_0 = roc_auc_score(sub_segment_0['intodefault_12'], y_pred_proba_0)\n",
    "    print(f\"AUC  for segment arrears=0 {sample_name} sample: {auc_0 :.4f}\")\n",
    "    \n",
    "    # Evaluate the model's performance\n",
    "    auc_1 = roc_auc_score(sub_segment_1['intodefault_12'], y_pred_proba_1)\n",
    "    print(f\"AUC  for segment arrears=1 {sample_name} sample: {auc_1 :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out_of_bounds_months = []\n",
    "\n",
    "for sample_name, sample in [('train', train), ('test', test), ('OOT', OOT)]:\n",
    "    y_pred_proba = logreg.predict_proba(sample.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1))[:, 1]\n",
    "    df_sample = sample.copy()\n",
    "    df_sample['PD'] = y_pred_proba\n",
    "    # Perform left join\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date']], on=['cid', 'duration'], how='left')\n",
    "    # Calculate sum of defaults and count of observations per reporting date\n",
    "    summary = merged_table.groupby('reporting_date').agg({'intodefault_12': ['sum', 'count']})\n",
    "    # Calculate default rate\n",
    "    summary['Default Rate'] = summary[('intodefault_12', 'sum')] / summary[('intodefault_12', 'count')]\n",
    "    # Calculate mean values per reporting date\n",
    "    summary['PD'] = merged_table.groupby('reporting_date')['PD'].mean()\n",
    "    summary['PD std'] = merged_table.groupby('reporting_date')['PD'].std()\n",
    "    # Calculate 95% HDI of posterior distribution using binomial model with Jeffreys prior\n",
    "    summary['jeffrey_p_value'], summary['HDI'] = zip(*summary.apply(lambda row: jeffreys_test(row['intodefault_12']['sum'], row['intodefault_12']['count'], row['PD']), axis=1))\n",
    "    # Extract lower and upper bounds from HDI\n",
    "    summary['LB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[0])\n",
    "    summary['UB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[1])\n",
    "    \n",
    "    summary.sort_values('reporting_date', inplace=True)\n",
    "    \n",
    "    # Plotting the default rate and PD\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size\n",
    "    plt.plot(summary.index, summary['Default Rate'], label='Default Rate', color='blue', marker='o')\n",
    "    plt.plot(summary.index, summary['PD'], label='PD', color='orange', marker='o')\n",
    "\n",
    "    # Plotting 95% HDI of posterior distribution\n",
    "    plt.fill_between(summary.index, summary['LB HDI 95%'], summary['UB HDI 95%'], linestyle='--', alpha=0.2, label='HDI 95%', color='blue')\n",
    "\n",
    "    # Manually set x-axis tick labels to display quarterly reporting date\n",
    "    quarterly_dates = pd.date_range(start=summary.index.min(), end=summary.index.max(), freq='Q')\n",
    "    plt.xticks(quarterly_dates, [format_quarterly_date(date) for date in quarterly_dates], rotation=45)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Reporting Date')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title(f'Into default Rate vs. PD over Time for sample {sample_name}')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # Display gridlines for better readability\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the graph\n",
    "    plt.tight_layout()  # Ensure the labels are not cut off\n",
    "    plt.show()\n",
    "    # Count the number of months the default rate is out of confidence bounds\n",
    "    out_of_bounds = np.where(\n",
    "        (summary['PD'] < summary['LB HDI 95%']) | (summary['PD'] > summary['UB HDI 95%'])\n",
    "    )[0]\n",
    "    out_of_bounds_months.append(len(out_of_bounds))\n",
    "\n",
    "    prob_true, prob_pred=plot_calibration_curve(merged_table['intodefault_12'], merged_table['PD'], f'Calibration curve sample {sample_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print the number of months the default rate is out of confidence bounds for each sample\n",
    "for i, sample_name in enumerate(['train', 'test', 'OOT']):\n",
    "    print(f\"Number of months out of bounds for {sample_name}: {out_of_bounds_months[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_name, sample in [('OOT', OOT_full)]:\n",
    "    # As an input to evaluate logistic regression results OOT_full is used as it contains observed loss information for 2021 data\n",
    "    y_pred_proba = logreg.predict_proba(sample.drop(['intodefault_12', 'default_adj', 'duration', 'cid'], axis=1))[:, 1]\n",
    "    df_sample = sample.copy()\n",
    "    df_sample['PD'] = y_pred_proba\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Outstanding']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "    merged_table['EL'] = merged_table['PD'] * merged_table['Outstanding']/1000000\n",
    "    merged_table['OL'] = merged_table.loc[merged_table['default_adj'] == 1, 'Outstanding']/1000000\n",
    "    merged_table['OL'] = merged_table['OL'].fillna(0)\n",
    "\n",
    "    expected_loss = merged_table.groupby('reporting_date')['EL'].sum().rename('Expected Loss').reset_index()\n",
    "    observed_loss = merged_table.groupby('reporting_date')['OL'].sum().rename('Observed Loss').reset_index()\n",
    "\n",
    "    observed_loss_comparison = observed_loss.copy()\n",
    "\n",
    "    # Add a new column for the corresponding observed loss one year after\n",
    "    observed_loss_comparison['Observed Loss (One Year After)'] = None\n",
    "\n",
    "    # Calculate the corresponding observed loss one year after for each reporting date\n",
    "    for i, row in observed_loss.iterrows():\n",
    "        current_date = row['reporting_date']\n",
    "        current_month = current_date.month\n",
    "        current_year = current_date.year\n",
    "\n",
    "        # Check for February 28th in a leap year\n",
    "        if current_month == 2 and current_date.day == 28 and current_year % 4 == 3:\n",
    "            # Calculate the corresponding date one year after, considering leap years\n",
    "            one_year_after = current_date + relativedelta(years=1, day=29)\n",
    "        else:\n",
    "            # Calculate the corresponding date one year after\n",
    "            one_year_after = current_date + relativedelta(years=1)\n",
    "\n",
    "        one_year_after_loss = observed_loss[observed_loss['reporting_date'] == one_year_after]['Observed Loss']\n",
    "\n",
    "        if not one_year_after_loss.empty:\n",
    "            observed_loss_comparison.at[i, 'One-Year Observed Loss'] = one_year_after_loss.iloc[0]\n",
    "\n",
    "    observed_loss_comparison = observed_loss_comparison[observed_loss_comparison['reporting_date'] < datetime(2021, 1, 1)]\n",
    "    expected_loss = expected_loss[expected_loss['reporting_date'] < datetime(2021, 1, 1)]\n",
    "    merged_table = merged_table[merged_table['reporting_date'] < datetime(2021, 1, 1)]\n",
    "\n",
    "    expected_loss.set_index('reporting_date', inplace=True)\n",
    "    observed_loss_comparison.set_index('reporting_date', inplace=True)\n",
    "\n",
    "    summary = pd.concat([expected_loss, observed_loss_comparison['One-Year Observed Loss']], axis=1)\n",
    "    summary['Difference'] = summary['Expected Loss'] - summary['One-Year Observed Loss']\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size\n",
    "    plt.plot(observed_loss_comparison.index, observed_loss_comparison['One-Year Observed Loss'], label='Observed Loss', marker='o')\n",
    "    plt.plot(expected_loss.index, expected_loss['Expected Loss'], label='Expected Loss', marker='o')\n",
    "\n",
    "    quarterly_dates = pd.date_range(start=summary.index.min(), end=summary.index.max(), freq='Q')\n",
    "    plt.xticks(quarterly_dates, [format_quarterly_date(date) for date in quarterly_dates], rotation=45)\n",
    "\n",
    "    plt.xlabel('Reporting Date')\n",
    "    plt.ylabel('Loss (in millions)')\n",
    "    plt.title(f'Expected Loss vs. Observed Loss over Time for sample {sample_name}')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Survival analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WeibullAFTFitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weibull AFT in lifelines library need duration time starting from 1 (not 0)\n",
    "T_train_on_adj=T_train_on+1\n",
    "T_test_on_adj=T_test_on+1\n",
    "T_test_OOT_adj=T_test_OOT+1\n",
    "timeline = np.arange(1, 361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waft = WeibullAFTFitter(penalizer=10)\n",
    "\n",
    "waft.fit(pd.concat([pd.DataFrame(X_train_on).reset_index(drop=True), T_train_on_adj.reset_index(drop=True), Y_train_on.reset_index(drop=True)], axis=1), 'duration', event_col='default_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "out_of_bounds_months = []\n",
    "auc_list = []\n",
    "c_index_list = []\n",
    "\n",
    "for sample_name, sample in [('train', train), ('test', test), ('OOT', OOT)]:\n",
    "    reset_sample = sample.reset_index(drop=True)\n",
    "    test_survival_probabilities = waft.predict_survival_function(reset_sample, times=np.arange(1, 361)).reset_index(drop=True)\n",
    "    \n",
    "    duration_column = reset_sample['duration']+12\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "    \n",
    "    df_sample=sample.reset_index(drop=True).copy()\n",
    "    df_sample['PD']=1-survival\n",
    "\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Arrear_amount_adj']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "    summary = merged_table.groupby('reporting_date').agg({'intodefault_12': ['sum', 'count']})\n",
    "    summary['Default Rate'] = summary[('intodefault_12', 'sum')] / summary[('intodefault_12', 'count')]\n",
    "    summary['PD'] = merged_table.groupby('reporting_date')['PD'].mean()\n",
    "    summary['PD std'] = merged_table.groupby('reporting_date')['PD'].std()\n",
    "    # Evaluate predictive ability\n",
    "    summary['jeffrey_p_value'], summary['HDI'] = zip(*summary.apply(lambda row: jeffreys_test(row['intodefault_12']['sum'], row['intodefault_12']['count'], row['PD']), axis=1))\n",
    "    summary['LB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[0])\n",
    "    summary['UB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[1])\n",
    "    \n",
    "    summary.sort_values('reporting_date', inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(summary.index, summary['Default Rate'], label='Default Rate', color='blue', marker='o')\n",
    "    plt.plot(summary.index, summary['PD'], label='PD', color='orange', marker='o')\n",
    "    plt.fill_between(summary.index, summary['LB HDI 95%'], summary['UB HDI 95%'], linestyle='--', alpha=0.2, color='blue', label=\"HDI\")\n",
    "\n",
    "    quarterly_dates = pd.date_range(start=summary.index.min(), end=summary.index.max(), freq='Q')\n",
    "    plt.xticks(quarterly_dates, [format_quarterly_date(date) for date in quarterly_dates], rotation=45)\n",
    "\n",
    "    plt.xlabel('Reporting date')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title(f'Into default Rate vs. PD over Time for sample {sample_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    out_of_bounds = np.where(\n",
    "        (summary['PD'] < summary['LB HDI 95%']) | (summary['PD'] > summary['UB HDI 95%'])\n",
    "    )[0]\n",
    "    out_of_bounds_months.append(len(out_of_bounds))\n",
    "    \n",
    "    # Evaluate the discriminatory power\n",
    "    auc = roc_auc_score(merged_table['intodefault_12'], merged_table['PD'])\n",
    "    auc_list.append(auc)\n",
    "    \n",
    "    c_index = concordance_index(merged_table['duration'], -merged_table['PD'], merged_table['intodefault_12'])\n",
    "    c_index_list.append(c_index)\n",
    "                                \n",
    "    plot_calibration_curve(merged_table['intodefault_12'], merged_table['PD'], f'Calibration curve sample {sample_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Print the number of months the default rate is out of confidence bounds for each sample\n",
    "for i, sample_name in enumerate(['train', 'test', 'OOT']):\n",
    "    print(f\"Number of months default rate is out of bounds for {sample_name}: {out_of_bounds_months[i]}\")\n",
    "    print(f\"AUC for {sample_name}: {auc_list[i]:.4f}\")\n",
    "    print(f\"C-index for {sample_name}: {c_index_list[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "merged_test_oot = pd.concat([test,OOT_full])\n",
    "result_table = pd.DataFrame()\n",
    "\n",
    "# Iterate over different reporting dates (December only)\n",
    "for reporting_date in pd.date_range(start='2013-12-01', end='2021-12-31', freq='12M'):\n",
    "    # Calculate the corresponding date for each time horizon of 0 to 8 years ahead\n",
    "    time_horizons = list(np.arange(0,9))\n",
    "    corresponding_dates = [reporting_date + relativedelta(years=t) for t in time_horizons]\n",
    "    corresponding_dates = [date for date in corresponding_dates if date < pd.Timestamp('2022-01-01')]\n",
    "    time_horizons = np.arange(len(corresponding_dates))\n",
    "    # Initialize lists to store the results for the current reporting date\n",
    "    reporting_dates_list = [reporting_date] * len(time_horizons)\n",
    "    time_horizons_list = time_horizons.copy()\n",
    "    corresponding_dates_list = corresponding_dates\n",
    "    expected_losses = []\n",
    "    observed_losses = []\n",
    "\n",
    "    for sample_name, sample in [('Merged', merged_test_oot)]:\n",
    "        reset_sample = sample.reset_index(drop=True)\n",
    "\n",
    "        test_survival_probabilities = waft.predict_survival_function(reset_sample, times=np.arange(1, 361)).reset_index(drop=True)\n",
    "\n",
    "        for years_ahead in time_horizons:\n",
    "            duration_column = reset_sample['duration'] + 12 * years_ahead\n",
    "            survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "\n",
    "            df_sample = sample.reset_index(drop=True).copy()\n",
    "            df_sample['PD'] = 1 - survival\n",
    "\n",
    "            merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Outstanding']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "            merged_table['EL'] = merged_table['PD'] * merged_table['Outstanding']/1000000\n",
    "\n",
    "            # Calculate Expected Loss for the current time horizon\n",
    "            expected_loss = merged_table.groupby('reporting_date')['EL'].sum().rename('Expected Loss').reset_index()\n",
    "            expected_loss = expected_loss[expected_loss['reporting_date'] == reporting_date]\n",
    "\n",
    "            # Calculate Observed Loss for the current time horizon\n",
    "            observed_loss = calculate_observed_loss(merged_table, reporting_date, years_ahead)\n",
    "\n",
    "            # Append the results to the lists\n",
    "            expected_losses.extend(expected_loss['Expected Loss'])\n",
    "            observed_losses.append(observed_loss/1000000)\n",
    "\n",
    "    # Append the results for the current reporting date to the result table\n",
    "    data = {\n",
    "        'Reporting Date': reporting_dates_list,\n",
    "        'Time Horizon': time_horizons_list,\n",
    "        'Corresponding Date': corresponding_dates_list,\n",
    "        'Expected Loss': expected_losses,\n",
    "        'Observed Loss': observed_losses\n",
    "    }\n",
    "    result_table = result_table.append(pd.DataFrame(data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogNormalAFTFitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognormaft = LogNormalAFTFitter(penalizer=10)\n",
    "\n",
    "lognormaft.fit(pd.concat([pd.DataFrame(X_train_on).reset_index(drop=True), T_train_on_adj.reset_index(drop=True), Y_train_on.reset_index(drop=True)], axis=1), 'duration', event_col='default_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_bounds_months = []\n",
    "auc_list = []\n",
    "c_index_list = []\n",
    "\n",
    "for sample_name, sample in [('train', train), ('test', test), ('OOT', OOT)]:\n",
    "    reset_sample = sample.reset_index(drop=True)\n",
    "    test_survival_probabilities = lognormaft.predict_survival_function(reset_sample, times=np.arange(1, 361)).reset_index(drop=True)\n",
    "    \n",
    "    duration_column = reset_sample['duration']+12\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "    \n",
    "    df_sample=sample.reset_index(drop=True).copy()\n",
    "    df_sample['PD']=1-survival\n",
    "\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Arrear_amount_adj']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "    summary = merged_table.groupby('reporting_date').agg({'intodefault_12': ['sum', 'count']})\n",
    "    summary['Default Rate'] = summary[('intodefault_12', 'sum')] / summary[('intodefault_12', 'count')]\n",
    "    summary['PD'] = merged_table.groupby('reporting_date')['PD'].mean()\n",
    "    summary['PD std'] = merged_table.groupby('reporting_date')['PD'].std()\n",
    "    # Evaluate predictive ability\n",
    "    summary['jeffrey_p_value'], summary['HDI'] = zip(*summary.apply(lambda row: jeffreys_test(row['intodefault_12']['sum'], row['intodefault_12']['count'], row['PD']), axis=1))\n",
    "    summary['LB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[0])\n",
    "    summary['UB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[1])\n",
    "    \n",
    "    summary.sort_values('reporting_date', inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(summary.index, summary['Default Rate'], label='Default Rate', color='blue', marker='o')\n",
    "    plt.plot(summary.index, summary['PD'], label='PD', color='orange', marker='o')\n",
    "    plt.fill_between(summary.index, summary['LB HDI 95%'], summary['UB HDI 95%'], linestyle='--', alpha=0.2, color='blue', label=\"HDI\")\n",
    "\n",
    "    quarterly_dates = pd.date_range(start=summary.index.min(), end=summary.index.max(), freq='Q')\n",
    "    plt.xticks(quarterly_dates, [format_quarterly_date(date) for date in quarterly_dates], rotation=45)\n",
    "\n",
    "    plt.xlabel('Reporting date')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title(f'Into default Rate vs. PD over Time for sample {sample_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    out_of_bounds = np.where(\n",
    "        (summary['PD'] < summary['LB HDI 95%']) | (summary['PD'] > summary['UB HDI 95%'])\n",
    "    )[0]\n",
    "    out_of_bounds_months.append(len(out_of_bounds))\n",
    "    \n",
    "    # Evaluate the discriminatory power\n",
    "    auc = roc_auc_score(merged_table['intodefault_12'], merged_table['PD'])\n",
    "    auc_list.append(auc)\n",
    "    \n",
    "    c_index = concordance_index(merged_table['duration'], -merged_table['PD'], merged_table['intodefault_12'])\n",
    "    c_index_list.append(c_index)\n",
    "                                \n",
    "    plot_calibration_curve(merged_table['intodefault_12'], merged_table['PD'], f'Calibration curve sample {sample_name}')\n",
    "    plt.show()\n",
    "\n",
    "for i, sample_name in enumerate(['train', 'test', 'OOT']):\n",
    "    print(f\"Number of months default rate is out of bounds for {sample_name}: {out_of_bounds_months[i]}\")\n",
    "    print(f\"AUC for {sample_name}: {auc_list[i]:.4f}\")\n",
    "    print(f\"C-index for {sample_name}: {c_index_list[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.DataFrame()\n",
    "\n",
    "# Iterate over different reporting dates (December only)\n",
    "for reporting_date in pd.date_range(start='2013-12-01', end='2021-12-31', freq='12M'):\n",
    "    time_horizons = list(np.arange(0,9))\n",
    "    corresponding_dates = [reporting_date + relativedelta(years=t) for t in time_horizons]\n",
    "    corresponding_dates = [date for date in corresponding_dates if date < pd.Timestamp('2022-01-01')]\n",
    "    time_horizons = np.arange(len(corresponding_dates))\n",
    "    reporting_dates_list = [reporting_date] * len(time_horizons)\n",
    "    time_horizons_list = time_horizons.copy()\n",
    "    corresponding_dates_list = corresponding_dates\n",
    "    expected_losses = []\n",
    "    observed_losses = []\n",
    "\n",
    "    for sample_name, sample in [('Merged', merged_test_oot)]:\n",
    "        reset_sample = sample.reset_index(drop=True)\n",
    "\n",
    "        test_survival_probabilities = lognormaft.predict_survival_function(reset_sample, times=np.arange(1, 361)).reset_index(drop=True)\n",
    "\n",
    "        for years_ahead in time_horizons:\n",
    "            duration_column = reset_sample['duration'] + 12 * years_ahead\n",
    "            survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "\n",
    "            df_sample = sample.reset_index(drop=True).copy()\n",
    "            df_sample['PD'] = 1 - survival\n",
    "            merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Outstanding']], on=['cid', 'duration'], how='left')\n",
    "            merged_table['EL'] = merged_table['PD'] * merged_table['Outstanding']/1000000\n",
    "\n",
    "            expected_loss = merged_table.groupby('reporting_date')['EL'].sum().rename('Expected Loss').reset_index()\n",
    "            expected_loss = expected_loss[expected_loss['reporting_date'] == reporting_date]\n",
    "            observed_loss = calculate_observed_loss(merged_table, reporting_date, years_ahead)\n",
    "            expected_losses.extend(expected_loss['Expected Loss']/1000000)\n",
    "            observed_losses.append(observed_loss)\n",
    "\n",
    "    data = {\n",
    "        'Reporting Date': reporting_dates_list,\n",
    "        'Time Horizon': time_horizons_list,\n",
    "        'Corresponding Date': corresponding_dates_list,\n",
    "        'Expected Loss': expected_losses,\n",
    "        'Observed Loss': observed_losses\n",
    "    }\n",
    "    result_table = result_table.append(pd.DataFrame(data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CoxPHFitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHFitter(penalizer=10)\n",
    "\n",
    "cph.fit(pd.concat([pd.DataFrame(X_train_on), T_train_on.reset_index(drop=True), Y_train_on.reset_index(drop=True)], axis=1), duration_col='duration', event_col='default_adj')\n",
    "\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_bounds_months = []\n",
    "auc_list = []\n",
    "c_index_list = []\n",
    "\n",
    "for sample_name, sample in [('train', train), ('test', test), ('OOT', OOT)]:\n",
    "    reset_sample = sample.reset_index(drop=True)\n",
    "    test_survival_probabilities = cph.predict_survival_function(reset_sample, times=np.arange(1, 361)).reset_index(drop=True)\n",
    "    \n",
    "    duration_column = reset_sample['duration']+12\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "    \n",
    "    df_sample=sample.reset_index(drop=True).copy()\n",
    "    df_sample['PD']=1-survival\n",
    "\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Arrear_amount_adj']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "    summary = merged_table.groupby('reporting_date').agg({'intodefault_12': ['sum', 'count']})\n",
    "    summary['Default Rate'] = summary[('intodefault_12', 'sum')] / summary[('intodefault_12', 'count')]\n",
    "    summary['PD'] = merged_table.groupby('reporting_date')['PD'].mean()\n",
    "    summary['PD std'] = merged_table.groupby('reporting_date')['PD'].std()\n",
    "    # Evaluate predictive ability\n",
    "    summary['jeffrey_p_value'], summary['HDI'] = zip(*summary.apply(lambda row: jeffreys_test(row['intodefault_12']['sum'], row['intodefault_12']['count'], row['PD']), axis=1))\n",
    "    summary['LB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[0])\n",
    "    summary['UB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[1])\n",
    "    \n",
    "    summary.sort_values('reporting_date', inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(summary.index, summary['Default Rate'], label='Default Rate', color='blue', marker='o')\n",
    "    plt.plot(summary.index, summary['PD'], label='PD', color='orange', marker='o')\n",
    "    plt.fill_between(summary.index, summary['LB HDI 95%'], summary['UB HDI 95%'], linestyle='--', alpha=0.2, color='blue', label=\"HDI\")\n",
    "\n",
    "    quarterly_dates = pd.date_range(start=summary.index.min(), end=summary.index.max(), freq='Q')\n",
    "    plt.xticks(quarterly_dates, [format_quarterly_date(date) for date in quarterly_dates], rotation=45)\n",
    "    \n",
    "    plt.xlabel('Reporting date')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title(f'Into default Rate vs. PD over Time for sample {sample_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    out_of_bounds = np.where(\n",
    "        (summary['PD'] < summary['LB HDI 95%']) | (summary['PD'] > summary['UB HDI 95%'])\n",
    "    )[0]\n",
    "    out_of_bounds_months.append(len(out_of_bounds))\n",
    "    \n",
    "    # Evaluate the discriminatory power\n",
    "    auc = roc_auc_score(merged_table['intodefault_12'], merged_table['PD'])\n",
    "    auc_list.append(auc)\n",
    "    \n",
    "    c_index = concordance_index(merged_table['duration'], -merged_table['PD'], merged_table['intodefault_12'])\n",
    "    c_index_list.append(c_index)\n",
    "                                \n",
    "    plot_calibration_curve(merged_table['intodefault_12'], merged_table['PD'], f'Calibration curve sample {sample_name}')\n",
    "    plt.show()\n",
    "\n",
    "for i, sample_name in enumerate(['train', 'test', 'OOT']):\n",
    "    print(f\"Number of months default rate is out of bounds for {sample_name}: {out_of_bounds_months[i]}\")\n",
    "    print(f\"AUC for {sample_name}: {auc_list[i]:.4f}\")\n",
    "    print(f\"C-index for {sample_name}: {c_index_list[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.DataFrame()\n",
    "\n",
    "# Iterate over different reporting dates (December only)\n",
    "for reporting_date in pd.date_range(start='2013-12-01', end='2021-12-31', freq='12M'):\n",
    "    time_horizons = list(np.arange(0,9))\n",
    "    corresponding_dates = [reporting_date + relativedelta(years=t) for t in time_horizons]\n",
    "    corresponding_dates = [date for date in corresponding_dates if date < pd.Timestamp('2022-01-01')]\n",
    "    time_horizons = np.arange(len(corresponding_dates))\n",
    "    reporting_dates_list = [reporting_date] * len(time_horizons)\n",
    "    time_horizons_list = time_horizons.copy()\n",
    "    corresponding_dates_list = corresponding_dates\n",
    "    expected_losses = []\n",
    "    observed_losses = []\n",
    "\n",
    "    for sample_name, sample in [('Merged', merged_test_oot)]:\n",
    "        reset_sample = sample.reset_index(drop=True)\n",
    "\n",
    "        test_survival_probabilities = cph.predict_survival_function(reset_sample, times=np.arange(1, 361)).reset_index(drop=True)\n",
    "\n",
    "        for years_ahead in time_horizons:\n",
    "            duration_column = reset_sample['duration'] + 12 * years_ahead\n",
    "            survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "\n",
    "            df_sample = sample.reset_index(drop=True).copy()\n",
    "            df_sample['PD'] = 1 - survival\n",
    "\n",
    "            merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Outstanding']], on=['cid', 'duration'], how='left')\n",
    "            merged_table['EL'] = merged_table['PD'] * merged_table['Outstanding']/1000000\n",
    "\n",
    "            expected_loss = merged_table.groupby('reporting_date')['EL'].sum().rename('Expected Loss').reset_index()\n",
    "            expected_loss = expected_loss[expected_loss['reporting_date'] == reporting_date]\n",
    "            observed_loss = calculate_observed_loss(merged_table, reporting_date, years_ahead)\n",
    "            expected_losses.extend(expected_loss['Expected Loss'])\n",
    "            observed_losses.append(observed_loss/1000000)\n",
    "\n",
    "    data = {\n",
    "        'Reporting Date': reporting_dates_list,\n",
    "        'Time Horizon': time_horizons_list,\n",
    "        'Corresponding Date': corresponding_dates_list,\n",
    "        'Expected Loss': expected_losses,\n",
    "        'Observed Loss': observed_losses\n",
    "    }\n",
    "    result_table = result_table.append(pd.DataFrame(data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeepHit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of model to be used during training\n",
    "duration_index = np.arange(1, 361)\n",
    "num_durations = len(duration_index)\n",
    "labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "get_target = lambda df: (df['duration'].values, df['default_adj'].values)\n",
    "y_train = labtrans.fit_transform(*get_target(train))\n",
    "y_val = labtrans.transform(*get_target(test))\n",
    "in_features = X_train_on.shape[1]\n",
    "out_features = labtrans.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertuning\n",
    "# Create space of hyperparameters values to serach\n",
    "space = {\n",
    "    'alpha': hp.uniform('alpha', 0, 0.5),\n",
    "    'sigma': hp.uniform('sigma', 0, 5),\n",
    "    'num_nodes': hp.quniform('num_nodes', 100, 5000, 100),\n",
    "    'num_layers': hp.quniform('num_layers', 1, 4, 1),  # Fix the list\n",
    "    'batch_norm': hp.choice('batch_norm', [True, False]),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0, 0.5),  # Fix the name\n",
    "    'batch_size': hp.quniform('batch_size', 100, 5000, 100)\n",
    "}\n",
    "\n",
    "# Define objective function\n",
    "def objective(params):\n",
    "    # Set seed\n",
    "    np.random.seed(123)\n",
    "    _ = torch.manual_seed(123)\n",
    "    \n",
    "    # Cast to an integer\n",
    "    params['num_nodes'] = int(params['num_nodes'])\n",
    "    params['num_layers'] = int(params['num_layers'])\n",
    "    params['batch_size'] = int(params['batch_size'])\n",
    "    \n",
    "    # Crate net\n",
    "    net = tt.practical.MLPVanilla(in_features, [params['num_nodes']] * params['num_layers'], out_features, params['batch_norm'], params['dropout_rate'])\n",
    "    model = DeepHitSingle(net, tt.optim.Adam, alpha=params['alpha'], sigma=params['sigma'], duration_index=np.arange(1, 361))\n",
    "    model.optimizer.set_lr(0.0001)\n",
    "    epochs = 5\n",
    "    callbacks = [tt.callbacks.EarlyStopping()]\n",
    "    log = model.fit(X_train_on.values.astype('float32'), (T_train_on.values, Y_train_on.values), params['batch_size'], epochs, callbacks, verbose=0, val_data=(X_test_on.values.astype('float32'), (T_test_on.values, Y_test_on.values)))\n",
    "    \n",
    "    train_survival_probabilities = model.predict_surv_df(X_test_OOT_full.values.astype('float32')).reset_index(drop=True)  # Reset index here\n",
    "    duration_column = OOT_full['duration'] + 12\n",
    "    survival = pd.Series(train_survival_probabilities.T.lookup(OOT_full.reset_index(drop=True).index, duration_column), index=OOT_full.reset_index(drop=True).index)\n",
    "\n",
    "    df_sample = OOT_full.reset_index(drop=True).copy()\n",
    "    df_sample['PD'] = 1 - survival\n",
    "    \n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Outstanding']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "    auc = roc_auc_score(merged_table['intodefault_12'], merged_table['PD'])\n",
    "    # Since the minimize function is used further, -auc is returned\n",
    "    return -auc\n",
    "\n",
    "# Run hyperparameter search\n",
    "trials = Trials()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print('Best hyperparameters:', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training of DeepHit model\n",
    "np.random.seed(123)\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "#net parameters\n",
    "num_nodes = 700\n",
    "num_layers = 1\n",
    "batch_norm = False\n",
    "dropout = 0.25\n",
    "net = tt.practical.MLPVanilla(in_features, [num_nodes]*num_layers, out_features, batch_norm, dropout)\n",
    "\n",
    "model = DeepHitSingle(net, tt.optim.Adam, alpha=0.4, sigma=2.5, duration_index=np.arange(1,361))\n",
    "model.optimizer.set_lr(0.0001)\n",
    "epochs = 10\n",
    "callbacks = [tt.callbacks.EarlyStopping()]\n",
    "log = model.fit(X_train_on.values.astype('float32'), (T_train_on.values,Y_train_on.values), 2900, epochs, callbacks, verbose=1, val_data=(X_test_on.values.astype('float32'), (T_test_on.values,Y_test_on.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out_of_bounds_months = []\n",
    "auc_list = []\n",
    "auc_0 = []\n",
    "\n",
    "for sample_name, sample, X in [('test', test, X_test_on), ('OOT', OOT, X_test_OOT)]:\n",
    "    reset_sample = sample.reset_index(drop=True)\n",
    "\n",
    "    test_survival_probabilities = model.predict_surv_df(X.values.astype('float32')).reset_index(drop=True)\n",
    "    \n",
    "    duration_column = reset_sample['duration']+12\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "    \n",
    "    df_sample=sample.reset_index(drop=True).copy()\n",
    "    df_sample['PD']=1-survival\n",
    "    # Perform left join to add reporting date and arrears information\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Arrear_amount_adj']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "    summary = merged_table.groupby('reporting_date').agg({'intodefault_12': ['sum', 'count']})\n",
    "    summary['Default Rate'] = summary[('intodefault_12', 'sum')] / summary[('intodefault_12', 'count')]\n",
    "    summary['PD'] = merged_table.groupby('reporting_date')['PD'].mean()\n",
    "    summary['PD std'] = merged_table.groupby('reporting_date')['PD'].std()\n",
    "    # Evaluate predictive ability\n",
    "    summary['jeffrey_p_value'], summary['HDI'] = zip(*summary.apply(lambda row: jeffreys_test(row['intodefault_12']['sum'], row['intodefault_12']['count'], row['PD']), axis=1))\n",
    "    summary['LB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[0])\n",
    "    summary['UB HDI 95%'] = summary['HDI'].apply(lambda hdi: hdi[1])\n",
    "    \n",
    "    summary.sort_values('reporting_date', inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(summary.index, summary['Default Rate'], label='Default Rate', color='blue', marker='o')\n",
    "    plt.plot(summary.index, summary['PD'], label='PD', color='orange', marker='o')\n",
    "    plt.fill_between(summary.index, summary['LB HDI 95%'], summary['UB HDI 95%'], linestyle='--', alpha=0.2, color='blue')\n",
    "\n",
    "    quarterly_dates = pd.date_range(start=summary.index.min(), end=summary.index.max(), freq='Q')\n",
    "    plt.xticks(quarterly_dates, [format_quarterly_date(date) for date in quarterly_dates], rotation=45)\n",
    "\n",
    "    plt.xlabel('Reporting date')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title(f'Into default Rate vs. PD over Time for sample {sample_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    out_of_bounds = np.where(\n",
    "        (summary['PD'] < summary['LB HDI 95%']) | (summary['PD'] > summary['UB HDI 95%'])\n",
    "    )[0]\n",
    "    out_of_bounds_months.append(len(out_of_bounds))\n",
    "    \n",
    "    # Evaluate the discriminatory power at the portfolio level\n",
    "    auc = roc_auc_score(merged_table['intodefault_12'], merged_table['PD'])\n",
    "    auc_list.append(auc)\n",
    "    \n",
    "    # Evaluate the discriminatory power at the material sub-ranges level\n",
    "    sub_segment_0 = merged_table[merged_table['Arrear_amount_adj'] == 0]\n",
    "    sub_segment_1 = merged_table[merged_table['Arrear_amount_adj'] > 0]\n",
    "    \n",
    "    auc_0 = roc_auc_score(sub_segment_0['intodefault_12'], sub_segment_0['PD'])\n",
    "    print(f\"AUC  for segment arrears=0 {sample_name} sample: {auc_0 :.4f}\")\n",
    "    auc_1 = roc_auc_score(sub_segment_1['intodefault_12'], sub_segment_1['PD'])\n",
    "    print(f\"AUC  for segment arrears=1 {sample_name} sample: {auc_1 :.4f}\")\n",
    "    \n",
    "    plot_calibration_curve(merged_table['intodefault_12'], merged_table['PD'], f'Calibration curve sample {sample_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for i, sample_name in enumerate(['test', 'OOT']):\n",
    "    print(f\"Number of months default rate is out of bounds for {sample_name}: {out_of_bounds_months[i]}\")\n",
    "    print(f\"AUC for {sample_name}: {auc_list[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AUC values over time for the first set of AUC values\n",
    "merged_test_oot_notfull = pd.concat([test,OOT])\n",
    "evaluation_results_1 = []\n",
    "evaluation_results_2 = []\n",
    "for samplename, sample in [('OOS and OOT combined', merged_test_oot_notfull)]:\n",
    "    auc_values_1 = []\n",
    "    auc_values_2 = []\n",
    "    df_sample = sample.copy()\n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date']], on=['cid', 'duration'], how='left')\n",
    "    \n",
    "    reporting_dates_to_evaluate = sorted(merged_table['reporting_date'].unique())\n",
    "\n",
    "    for reporting_date in reporting_dates_to_evaluate:\n",
    "        sample_time = merged_table[merged_table['reporting_date'] == reporting_date]\n",
    "        \n",
    "        if len(sample_time['intodefault_12'].unique()) == 1:\n",
    "            auc_values_1.append(None)\n",
    "            auc_values_2.append(None)\n",
    "        else:\n",
    "            auc, _ = evaluate_model_performance(sample_time, logreg, reporting=1)\n",
    "            auc_values_1.append(auc)\n",
    "            \n",
    "            auc, _ = evaluate_model_performance(sample_time, model, reporting=1)\n",
    "            auc_values_2.append(auc)\n",
    "\n",
    "    evaluation_results_1.append({\n",
    "        'dates': reporting_dates_to_evaluate,\n",
    "        'Sample': samplename,\n",
    "        'AUC Values': auc_values_1\n",
    "    })\n",
    "    evaluation_results_2.append({\n",
    "        'dates': reporting_dates_to_evaluate,\n",
    "        'Sample': samplename,\n",
    "        'AUC Values': auc_values_2\n",
    "    })\n",
    "\n",
    "# Combine the two sets of AUC values into one plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for evaluation in evaluation_results_1:\n",
    "    plt.plot(evaluation['dates'], evaluation['AUC Values'], label=f'Logistic regression AUC', marker='o')\n",
    "for evaluation in evaluation_results_2:\n",
    "    plt.plot(evaluation['dates'], evaluation['AUC Values'], label=f'DeepHit AUC', marker='o') \n",
    "\n",
    "oot_threshold_date=datetime(2019,7,31)\n",
    "\n",
    "plt.axvspan(reporting_dates_to_evaluate[0], oot_threshold_date, facecolor='lightgray', alpha=0.5)\n",
    "plt.axvspan(oot_threshold_date, reporting_dates_to_evaluate[-1], facecolor='lightcoral', alpha=0.5)\n",
    "\n",
    "# Add a horizontal line for July 2019\n",
    "plt.axvline(x=oot_threshold_date, color='gray', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add text to indicate OOT and OOS\n",
    "plt.text(pd.to_datetime('2019-08-31'), 0.86, 'OOT', fontsize=12, color='black')\n",
    "plt.text(pd.to_datetime('2019-03-01'), 0.86, 'OOS', fontsize=12, color='black')\n",
    "\n",
    "plt.xlabel('Reporting Date')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC for Different Reporting Dates over Time')\n",
    "plt.xlim(reporting_dates_to_evaluate[0],reporting_dates_to_evaluate[-1])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of durations to evaluate\n",
    "durations_to_evaluate = [12, 24, 36, 48, 60]\n",
    "\n",
    "# AUC values over time for the second set of AUC values\n",
    "evaluation_results_1 = []\n",
    "evaluation_results_2 = []\n",
    "for samplename, sample in [('OOT', OOT)]:\n",
    "    auc_values_1 = []\n",
    "    auc_values_2 = []\n",
    "    for duration in durations_to_evaluate:\n",
    "        # Filter data based on the duration\n",
    "        sample_time = sample[sample['duration'] == duration]\n",
    "        \n",
    "        # Evaluate the model's performance for the specific duration at the specific reporting_date\n",
    "        auc, _ = evaluate_model_performance(sample_time, logreg)\n",
    "        # Append the AUC value to the list for the specific duration \n",
    "        auc_values_1.append(auc)\n",
    "        \n",
    "        auc, _ = evaluate_model_performance(sample_time, model)\n",
    "        auc_values_2.append(auc)\n",
    "\n",
    "    # Append the results to the evaluation_results list\n",
    "    evaluation_results_1.append({\n",
    "        'duration': duration,\n",
    "        'Sample': sample_name,\n",
    "        'AUC Values': auc_values_1\n",
    "    })\n",
    "    evaluation_results_2.append({\n",
    "        'duration': duration,\n",
    "        'Sample': sample_name,\n",
    "        'AUC Values': auc_values_2\n",
    "    })\n",
    "    \n",
    "# Combine the two sets of AUC values into one plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the AUC for different durations over time\n",
    "for evaluation in evaluation_results_1:\n",
    "    plt.plot(durations_to_evaluate, evaluation['AUC Values'], label=f'Logistic regression AUC', marker='o')\n",
    "for evaluation in evaluation_results_2:\n",
    "    plt.plot(durations_to_evaluate, evaluation['AUC Values'], label=f'DeepHit AUC', marker='o')\n",
    "\n",
    "plt.xlabel('duration')\n",
    "plt.xticks([12, 24, 36, 48, 60])\n",
    "plt.ylabel('AUC')\n",
    "plt.title(f'AUC for Different Durations over Time for sample {evaluation[\"Sample\"]}')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECL vs OL over one-year horizon on OOT evaluation\n",
    "for sample_name, sample in [('OOT', OOT_full)]:\n",
    "    reset_sample = sample.reset_index(drop=True)\n",
    "\n",
    "    test_survival_probabilities = model.predict_surv_df(X_test_OOT_full.values.astype('float32')).reset_index(drop=True)\n",
    "    \n",
    "    duration_column = reset_sample['duration']+12\n",
    "    survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "    \n",
    "    df_sample=sample.reset_index(drop=True).copy()\n",
    "    df_sample['PD']=1-survival\n",
    "    \n",
    "    merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Outstanding']], on=['cid', 'duration'], how='left')\n",
    "\n",
    "    merged_table['EL'] = merged_table['PD'] * (merged_table['Outstanding']-12*merged_table['Outstanding']/(360-reset_sample['duration']))/1000000\n",
    "    merged_table['OL'] = merged_table.loc[merged_table['default_adj'] == 1, 'Outstanding']/1000000\n",
    "    merged_table['OL'] = merged_table['OL'].fillna(0)\n",
    "\n",
    "    expected_loss = merged_table.groupby('reporting_date')['EL'].sum().rename('Expected Loss').reset_index()\n",
    "    observed_loss = merged_table.groupby('reporting_date')['OL'].sum().rename('Observed Loss').reset_index()\n",
    "\n",
    "    observed_loss_comparison = observed_loss.copy()\n",
    "    observed_loss_comparison['Observed Loss (One Year After)'] = None\n",
    "\n",
    "    for i, row in observed_loss.iterrows():\n",
    "        current_date = row['reporting_date']\n",
    "        current_month = current_date.month\n",
    "        current_year = current_date.year\n",
    "\n",
    "        if current_month == 2 and current_date.day == 28 and current_year % 4 == 3:\n",
    "            one_year_after = current_date + relativedelta(years=1, day=29)\n",
    "        else:\n",
    "            one_year_after = current_date + relativedelta(years=1)\n",
    "\n",
    "        one_year_after_loss = observed_loss[observed_loss['reporting_date'] == one_year_after]['Observed Loss']\n",
    "\n",
    "        if not one_year_after_loss.empty:\n",
    "            observed_loss_comparison.at[i, 'One-Year Observed Loss'] = one_year_after_loss.iloc[0]\n",
    "\n",
    "    observed_loss_comparison = observed_loss_comparison[observed_loss_comparison['reporting_date'] < datetime(2021, 1, 1)]\n",
    "    expected_loss = expected_loss[expected_loss['reporting_date'] < datetime(2021, 1, 1)]\n",
    "    merged_table = merged_table[merged_table['reporting_date'] < datetime(2021, 1, 1)]\n",
    "\n",
    "    # Combine expected_loss, observed_loss_comparison, and the difference between the two\n",
    "    summary = pd.concat([expected_loss, observed_loss_comparison['One-Year Observed Loss']], axis=1)\n",
    "    summary['Difference'] = summary['Expected Loss'] - summary['One-Year Observed Loss']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(observed_loss_comparison['reporting_date'], observed_loss_comparison['One-Year Observed Loss'], label='Observed Loss', marker='o')\n",
    "    plt.plot(expected_loss['reporting_date'], expected_loss['Expected Loss'], label='Expected Loss', marker='o')\n",
    "\n",
    "    quarterly_dates = pd.date_range(start=expected_loss['reporting_date'].min(), end=expected_loss['reporting_date'].max(), freq='Q')\n",
    "    plt.xticks(quarterly_dates, [format_quarterly_date(date) for date in quarterly_dates], rotation=45)\n",
    "\n",
    "    plt.xlabel('Reporting date')\n",
    "    plt.ylabel('Loss (in M EUR)')\n",
    "    plt.title(f'Expected Loss vs. Observed Loss over Time for sample {sample_name}')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECL vs OL evaluation\n",
    "result_table = pd.DataFrame()\n",
    "merged_X_test = pd.concat([X_test_on,X_test_OOT_full])\n",
    "\n",
    "for reporting_date in pd.date_range(start='2013-12-01', end='2021-12-31', freq='12M'):\n",
    "    time_horizons = list(np.arange(0,9))\n",
    "    corresponding_dates = [reporting_date + relativedelta(years=t) for t in time_horizons]\n",
    "    corresponding_dates = [date for date in corresponding_dates if date < pd.Timestamp('2022-01-01')]\n",
    "    time_horizons = np.arange(len(corresponding_dates))\n",
    "    reporting_dates_list = [reporting_date] * len(time_horizons)\n",
    "    time_horizons_list = time_horizons.copy()\n",
    "    corresponding_dates_list = corresponding_dates\n",
    "    expected_losses = []\n",
    "    observed_losses = []\n",
    "\n",
    "    for sample_name, sample in [('Merged', merged_test_oot)]:\n",
    "        reset_sample = sample.reset_index(drop=True)\n",
    "\n",
    "        test_survival_probabilities = model.predict_surv_df(merged_X_test.values.astype('float32')).reset_index(drop=True)\n",
    "\n",
    "        for years_ahead in time_horizons:\n",
    "            duration_column = reset_sample['duration'] + 12 * years_ahead\n",
    "            survival = pd.Series(test_survival_probabilities.T.lookup(reset_sample.index, duration_column), index=reset_sample.index)\n",
    "\n",
    "            df_sample = sample.reset_index(drop=True).copy()\n",
    "            df_sample['PD'] = 1 - survival\n",
    "\n",
    "            merged_table = pd.merge(df_sample, df[['cid', 'duration', 'reporting_date', 'Outstanding']], on=['cid', 'duration'], how='left')\n",
    "            merged_table['EL'] = merged_table['PD'] * merged_table['Outstanding']/1000000\n",
    "\n",
    "            expected_loss = merged_table.groupby('reporting_date')['EL'].sum().rename('Expected Loss').reset_index()\n",
    "            expected_loss = expected_loss[expected_loss['reporting_date'] == reporting_date]\n",
    "            observed_loss = calculate_observed_loss(merged_table, reporting_date, years_ahead)\n",
    "\n",
    "            expected_losses.extend(expected_loss['Expected Loss'])\n",
    "            observed_losses.append(observed_loss/1000000)\n",
    "\n",
    "    data = {\n",
    "        'Reporting Date': reporting_dates_list,\n",
    "        'Time Horizon': time_horizons_list,\n",
    "        'Corresponding Date': corresponding_dates_list,\n",
    "        'Expected Loss': expected_losses,\n",
    "        'Observed Loss': observed_losses\n",
    "    }\n",
    "    result_table = result_table.append(pd.DataFrame(data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate through time horizon\n",
    "for time_horizon in range(9):\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(result_table[result_table['Time Horizon']==time_horizon]['Reporting Date'], result_table[result_table['Time Horizon']==time_horizon]['Observed Loss'], label='Observed Loss', marker='o')\n",
    "    plt.plot(result_table[result_table['Time Horizon']==time_horizon]['Reporting Date'], result_table[result_table['Time Horizon']==time_horizon]['Expected Loss'], label='Expected Loss', marker='o')\n",
    "\n",
    "    plt.xlabel('Reporting date')\n",
    "    plt.xticks(result_table[result_table['Time Horizon']==time_horizon]['Reporting Date'].unique())\n",
    "    plt.ylabel('Loss (in M EUR)')\n",
    "    plt.title(f'Expected Loss vs. Observed Loss over Time for {time_horizon}-year time horizon OOS and OOT combined')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Correlation of PD predictions obtained from different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Overall\n",
    "for sample_name, sample in [('test', test), ('OOT', OOT)]:\n",
    "    \n",
    "    #Results\n",
    "    corr_matrix, p_value, DeepHit_PD, LR_PD, waft_PD, lognormaft_PD, cph_PD = calculate_corr(sample, sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the PD models and their corresponding labels\n",
    "models = [LR_PD, DeepHit_PD, lognormaft_PD, waft_PD, cph_PD]\n",
    "model_labels = ['Logistic regression PD', 'DeepHit PD', 'Log-Normal AFT PD', 'Weibull AFT', 'Cox PH']\n",
    "\n",
    "# Iterate through the models and create scatterplots and calculate Spearman coefficients\n",
    "for i in range(len(models)):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(OOT['duration'], models[i])\n",
    "    plt.xlabel('duration')\n",
    "    plt.ylabel(model_labels[i])\n",
    "    plt.title(f'Scatterplot between duration of facility in the portfolio and {model_labels[i]} on OOT sample')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    corr_coef, _ = stats.spearmanr(OOT['duration'], models[i])\n",
    "    print(f\"Spearman coefficient between duration of facility in the portfolio and {model_labels[i]} for OOT sample is equal to {corr_coef:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation of AUC values over duration of client in the portfolio\n",
    "evaluation_results = []\n",
    "models_to_evaluate = ['Logistic regression', 'Weibull AFT', 'Log-Normal AFT', 'Cox PH']\n",
    "\n",
    "for sample_name, sample in [('test', test), ('OOT', OOT)]:\n",
    "    corr_values = []\n",
    "\n",
    "    for duration in durations_to_evaluate:\n",
    "        sample_time = sample[sample['duration'] == duration]\n",
    "\n",
    "        corr_matrix, _, _, _, _, _, _ = calculate_corr(sample_time, sample_name, duration_ind=1)\n",
    "\n",
    "        for model_name in models_to_evaluate:\n",
    "            corr_value = corr_matrix[models_to_evaluate.index(model_name), 4]\n",
    "\n",
    "            # Append correlation values to the list\n",
    "            evaluation_results.append({\n",
    "                'Model': model_name,\n",
    "                'Sample': sample_name,\n",
    "                'Duration': duration,\n",
    "                'Corr Value': corr_value\n",
    "            })\n",
    "\n",
    "for sample_name in ['OOT', 'test']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Iterate through the models and create scatterplots\n",
    "    for model_name in models_to_evaluate:\n",
    "        # Filter the evaluation results for the current sample and model\n",
    "        filtered_results = [item for item in evaluation_results if item['Sample'] == sample_name and item['Model'] == model_name]\n",
    "        \n",
    "        # Extract duration and correlation values\n",
    "        durations = [item['Duration'] for item in filtered_results]\n",
    "        corr_values = [item['Corr Value'] for item in filtered_results]\n",
    "        \n",
    "        # Plot the correlation values for the current model\n",
    "        plt.plot(durations, corr_values, label=model_name, marker='o')\n",
    "\n",
    "    plt.xlabel('Duration (months)')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.title(f'Correlation for Different Durations over Time for {sample_name}')\n",
    "    plt.xticks(durations_to_evaluate)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset warnings settings to default\n",
    "warnings.filterwarnings(\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
